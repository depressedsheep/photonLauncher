#!/usr/bin/env python

"""
Script to get a quick and dirty evaluation of the heralded efficiency in a coincidence count scheme.
It implements a class to manage the data generated by the timestamp card into a G2 correlation function.

July 2014
Alessandro Cere

Corr_efficiency function was edited to take into account background count rate of channel A.
The rate is input manually via the input modifer -b in units of per second.

kiv: 
1. currently error for background in channel A is treated as Poissonian (sqrtB)
  ie. B = b*t
  where B = background counts within acquisition time t.
	b = background count rate measured seperately.
, however we have yet to consider error in background count rate itself.

Dec 2014
Jianwei

"""

import subprocess
import datetime
import time
import argparse

import matplotlib.pyplot as plt
import numpy as np


class G2():
    # command for the C data parser
    cmd = '/home/qitlab/programs/g2/g2_test'
    # cmd = '/home/qitlab/programs/g2/g2_lepton'

    def __init__(self, raw_filename, binning=8, length=600, out_filename=None):
        """
        Read from the raw timestamp file into a processed histogram object.
        It uses the C data parser specified in cmd.
        :param out_filename: filename for the output of the g2 parser
        :param raw_filename: filename of the raw data
        :param binning: time duration of the bins expressed in 1/8 of ns
        :param length: total number of bins
        """
        # Check the output filename and assign a default one if not defined
        if out_filename is None:
            out_filename = raw_filename.split('.')[0] + '.g2'

        # options for the C parser
        opts = (' -i ' + raw_filename + ' -t {} -m {} -o ' + out_filename).format(binning, length)

        # here we call the parser with the appropriate options
        subprocess.call(self.cmd + opts, stderr=subprocess.STDOUT, stdout=subprocess.PIPE, shell=True)

        # from the header we can extract some useful information
        with open(out_filename, 'r') as f:
            first_line = f.readline().replace(',', ' ').split()
            second_line = f.readline().replace(',', ' ').split()
        self.counts_0 = int(first_line[2])
        self.counts_1 = int(first_line[4])
        self.total_time = int(second_line[2]) / 8. * 1e-9
        self.bin_time = int(second_line[4]) / 8.
        self.rate_0 = self.counts_0 / self.total_time
        self.rate_1 = self.counts_1 / self.total_time
        self.coincidence_rate_1 = (self.rate_1) ** 2 * ((binning * 1e-9) / 8)
        self.coincidence_counts = self.coincidence_rate_1 * self.total_time
        print('Total acquisition time = {0:.2f} s'.format(self.total_time))
        print('Channel A total counts = {0}'.format(self.counts_0))
        print('Channel A count rate = {0:.2f} per sec'.format(self.counts_0 / self.total_time))
        print('Channel B count rate = {0:.2f} per sec'.format(self.counts_1 / self.total_time))
        print('ChB Auto coincidence counts = {0:.2f}'.format(self.coincidence_counts))
        # histogram for the cross correlation between channels A and B and autocorrelation of channel B
        self.data = np.genfromtxt(out_filename, skip_header=2)
        self.delta_t = self.data[:, 0]
        self.cross_corr = self.data[:, 1]
        self.auto_corr = self.data[:, 2]

    def peak_integrate(self, t_width):
        """
        Function to integrate the peak of the histogram over the specified time width.
        :param t_width: temporal width
        :return: an int result of the integration.
        """
        # find the index of the maximum of the histo and the corresponding time
        self.max_idx, p_value = np.argmax(self.cross_corr), max(self.cross_corr)
        n_bin = int(t_width / self.bin_time / 2)

        # return the integral of the peak and the associated error, assuming a poissonian distribution
        return np.sum(self.cross_corr[self.max_idx - n_bin:self.max_idx + n_bin]), \
               np.sqrt(np.sum(self.cross_corr[self.max_idx - n_bin:self.max_idx + n_bin]))

    def noise(self, t_width):
        """
        Looks for a relatively flat area of the histogram and uses it to estimate the "noise", or accidental counts.
        This function is meant to be used after peak_integrate to correct the efficiency for accidental counts.
        :param t_width: temporal width
        :return: an int result of the integration
        """
        # find the peak and look for a value away from it
        self.noise_idx = (len(self.delta_t) - self.max_idx) / 2
        n_bin = int(t_width / self.bin_time / 2)

        return np.sum(self.cross_corr[self.noise_idx - n_bin:self.noise_idx + n_bin]), \
               np.sqrt(np.sum(self.cross_corr[self.noise_idx - n_bin:self.noise_idx + n_bin]))

    def efficiency(self, t_width):
        signal, sig_error = self.peak_integrate(t_width)
        eff = float(signal / self.counts_0)
        error_eff = np.sqrt(signal * (1 + eff)) / self.counts_0
        n_bin = int(t_width / self.bin_time / 2)

        return eff, error_eff, self.max_idx - n_bin, self.max_idx + n_bin

    def corr_efficiency(self, t_width):
        """
        Corrected heralded efficiency. It returns the efficiency and the indices of the beginning and end
        of the coincidence window considered.
        :param t_width: Width of the coincidence window in ns.
        :return:
        """
        signal, sig_error = self.peak_integrate(t_width)
        noise, err_error = self.noise(t_width)
        bkgrd = args.b * self.total_time  # background in Channel A(trigger)
        # eff = (signal-noise)/self.counts_0
        eff = (signal - noise) / (self.counts_0 - bkgrd)
        print('Coincidences minus accidentals = {0}'.format(signal - noise))
        # error_eff = np.sqrt((signal+noise) * (1+eff)) / self.counts_0

        error_eff = np.sqrt(
            (self.counts_0 - bkgrd) ** 2 * (signal + noise) + (signal - noise) ** 2 * (self.counts_0 + bkgrd)) / (
                                                                                                                     self.counts_0 - bkgrd) ** 2

        n_bin = int(t_width / self.bin_time / 2)

        return eff, error_eff, self.max_idx - n_bin, self.max_idx + n_bin


def data_acq(acq_time, filename):
    """
    Function to start the acquisition of data by the timestamp card. the process is then stopped
    after the specified time.
    :param acq_time: acquisition time
    :param filename: name of the file where to store the raw data
    """
    cmd = '/home/qitlab/programs/timestamp3/readevents3'
    with open(filename, 'w') as f:
        proc = subprocess.Popen([cmd, '-a1', '-e'], stdout=f)
        time.sleep(acq_time)
        proc.kill()


def plot_g2(raw_filename, width):
    """
    Function to plot the cross correlation and the autocorrelation from a RAW file generated by the timestamp card.
    :param raw_filename:
    :param width: Coincidence time window in ns
    """
    g2 = G2(raw_filename)

    # plot of the cross correlation
    plt.figure('Cross correlation with coincidence window highlighted')
    # plt.plot(g2.delta_t, g2.cross_corr, 'ob')
    plt.plot(g2.delta_t, g2.cross_corr)
    np.savetxt(raw_filename + '_g2.txt', zip(g2.delta_t, g2.cross_corr))

    # Calculates the corrected efficiency
    eff, eff_error, p_min, p_max = g2.corr_efficiency(width)
    print('Corrected efficiency = {0:.2f} +/- {1:.2f} %'.format(eff * 100, eff_error * 100))
    if args.acq:
        print('Timestamp = ' + str(datetime.datetime.now()))
    plt.semilogy(g2.delta_t[p_min:p_max], g2.cross_corr[p_min:p_max], 'r')
    plt.xlim([200, np.max(g2.delta_t)])

    # plot of the autocorrelation
    plt.figure('Autocorrelation of channel B')
    plt.plot(g2.delta_t, g2.auto_corr, 'og')
    plt.axhline(y=g2.coincidence_counts)


if __name__ == '__main__':
    # let's take care of the CLI arguments:
    parser = argparse.ArgumentParser(description='Script to evaluate the coincidence counts'
                                                 'from the RAW timestamp data file.')
    parser.add_argument('inputfile', metavar='file name', type=str, help='Input RAW file', default='test.raw')
    parser.add_argument('-acq', action="store_true", help='tells the script to acquire data ')
    parser.add_argument('-t', metavar='acquisition time', type=int, default=1, help='Acquisition time in seconds')
    parser.add_argument('-b', metavar='bkgdcr_chA', type=float, default=0,
                        help='Channel A background count rate (per sec)')
    parser.add_argument('-p', type=int, default=1, help='show plots')
    args = parser.parse_args()

    inputfile = args.inputfile
    if args.acq:
        data_acq(args.t, inputfile)
    plot_g2(inputfile, 150)
    if args.p == 1:
        plt.show()